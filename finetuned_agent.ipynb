{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from   matplotlib import colors\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from glob import glob\n",
    "\n",
    "import langchain # Main LangChain import\n",
    "from langchain_openai import ChatOpenAI # To work with OpenAI\n",
    "# from langchain_anthropic import ChatAnthropic # To work with Anthropic (optional)\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI # To work with Gemini (optional)\n",
    "from langchain_core.output_parsers import JsonOutputParser # To help with structured output\n",
    "from langchain_core.prompts import PromptTemplate # To help create our prompt\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # To help with defining what output structure we want\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Loading Files\n",
    "# ==========================\n",
    "\n",
    "base_path = 'data/challenges/'\n",
    "\n",
    "# Subset of challenges for testing\n",
    "subset_ids_challenges = load_json(base_path + '28_8x10_challenges.json')\n",
    "subset_ids_evaluation = load_json(base_path + '28_15x15_evaluation.json')\n",
    "\n",
    "# Load evaluation dataset\n",
    "evaluation_challenges = load_json(base_path + 'arc-agi_evaluation_challenges.json')\n",
    "evaluation_solutions = load_json(base_path + 'arc-agi_evaluation_solutions.json')\n",
    "\n",
    "# load training dataset\n",
    "training_challenges = load_json(base_path + 'arc-agi_training_challenges.json')\n",
    "training_solutions = load_json(base_path + 'arc-agi_training_solutions.json')\n",
    "\n",
    "# Filter training challenges and solutions to only include the subset IDs\n",
    "training_challenges = {k: v for k, v in training_challenges.items() if k in subset_ids_challenges}\n",
    "training_solutions = {k: v for k, v in training_solutions.items() if k in subset_ids_challenges}\n",
    "\n",
    "# filter evaluation challenges and solutions to only include the subset IDs\n",
    "evaluation_challenges = {k: v for k, v in evaluation_challenges.items() if k in subset_ids_evaluation}\n",
    "evaluation_solutions = {k: v for k, v in evaluation_solutions.items() if k in subset_ids_evaluation}\n",
    "\n",
    "test_challenges = load_json(base_path + 'arc-agi_test_challenges.json')\n",
    "\n",
    "\n",
    "\n",
    "task_sets = {\n",
    "    'training': {\n",
    "        'challenges': training_challenges,\n",
    "        'solutions': training_solutions,\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'challenges': evaluation_challenges,\n",
    "        'solutions': evaluation_solutions,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Function to load tasks from a pre-loaded task set\n",
    "# ==========================\n",
    "\n",
    "def load_tasks_from_file(task_set):\n",
    "    \"\"\"\n",
    "    Loads the tasks from the pre-loaded JSON data and returns the challenges and solutions tasks.\n",
    "    \"\"\"\n",
    "    challenges = task_set['challenges']\n",
    "    solutions = task_set['solutions']\n",
    "\n",
    "    return challenges, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training challenges = 400\n",
      "Number of solutions of training challenges = 400\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training challenges = {len(training_challenges)}')\n",
    "print(f'Number of solutions of training challenges = {len(training_solutions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': [{'input': [[1, 0, 1, 5, 1, 0, 1], [0, 1, 0, 5, 1, 0, 1], [1, 0, 1, 5, 0, 1, 0]]}], 'train': [{'input': [[1, 0, 0, 5, 0, 1, 0], [0, 1, 0, 5, 1, 1, 1], [1, 0, 0, 5, 0, 0, 0]], 'output': [[0, 0, 0], [0, 2, 0], [0, 0, 0]]}, {'input': [[1, 1, 0, 5, 0, 1, 0], [0, 0, 1, 5, 1, 1, 1], [1, 1, 0, 5, 0, 1, 0]], 'output': [[0, 2, 0], [0, 0, 2], [0, 2, 0]]}, {'input': [[0, 0, 1, 5, 0, 0, 0], [1, 1, 0, 5, 1, 0, 1], [0, 1, 1, 5, 1, 0, 1]], 'output': [[0, 0, 0], [2, 0, 0], [0, 0, 2]]}]}\n"
     ]
    }
   ],
   "source": [
    "# Loading tasks from the 'training' task set\n",
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "print(challenges['0520fde7'])  # Accessing a specific challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initializing LLM client to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('api.env')\n",
    "\n",
    "# Get the OpenAI API key from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the ChatOpenAI model with the API key\n",
    "#llm = ChatOpenAI(model='ft:gpt-4o-mini-2024-07-18:personal:arc-agi-4omini:AVHPjOLN', openai_api_key=openai_api_key, max_tokens=3000)\n",
    "llm=ChatOpenAI(model='ft:gpt-4o-2024-08-06:personal:4o-arc-agi:AXmR1Dqz', openai_api_key=openai_api_key, max_tokens=3000)\n",
    "\n",
    "## And incase you want to try Anthropic\n",
    "# llm = ChatAnthropic(model='claude-3-5-sonnet-20240620', api_key=UserSecretsClient().get_secret(\"ANTHROPIC_API_KEY\"), max_tokens=3000)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"), max_tokens=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to make MVP product which is just regular openai model trying to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting train and test pairs to a string format ideal for LLMs\n",
    "def json_task_to_string(challenge_tasks: dict, task_id: str, test_input_index: int) -> str:\n",
    "    \"\"\"\n",
    "    challenge_tasks: dict a list of tasks\n",
    "    task_id: str the id of the task we want to convert to a string\n",
    "    \n",
    "    Convert your json task into a string so you can pass it to your LLM.\n",
    "    This is a crucial step where you can use your creativity to edit how tasks are represented.\n",
    "    \"\"\"\n",
    "    json_task = challenge_tasks[task_id]\n",
    "\n",
    "    final_output = \"\"\n",
    "\n",
    "    train_tasks = json_task['train']\n",
    "    test_task = json_task['test']\n",
    "\n",
    "    final_output = \"Training Examples\\n\"\n",
    "\n",
    "    for i, task in enumerate(train_tasks):\n",
    "        final_output += f\"Example {i + 1}: Input\\n[\"\n",
    "        for row in task['input']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "        final_output += f\"Example {i + 1}: Output\\n[\"\n",
    "\n",
    "        for row in task['output']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "\n",
    "    final_output += \"Test\\n[\"\n",
    "    for row in test_task[test_input_index]['input']:\n",
    "        final_output += f\"\\n{str(row)}\"\n",
    "\n",
    "    final_output += \"]\\n\\nYour Response:\"\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Examples\n",
      "Example 1: Input\n",
      "[\n",
      "[1, 0, 0, 5, 0, 1, 0],\n",
      "[0, 1, 0, 5, 1, 1, 1],\n",
      "[1, 0, 0, 5, 0, 0, 0],]\n",
      "\n",
      "Example 1: Output\n",
      "[\n",
      "[0, 0, 0],\n",
      "[0, 2, 0],\n",
      "[0, 0, 0],]\n",
      "\n",
      "Example 2: Input\n",
      "[\n",
      "[1, 1, 0, 5, 0, 1, 0],\n",
      "[0, 0, 1, 5, 1, 1, 1],\n",
      "[1, 1, 0, 5, 0, 1, 0],]\n",
      "\n",
      "Example 2: Output\n",
      "[\n",
      "[0, 2, 0],\n",
      "[0, 0, 2],\n",
      "[0, 2, 0],]\n",
      "\n",
      "Example 3: Input\n",
      "[\n",
      "[0, 0, 1, 5, 0, 0, 0],\n",
      "[1, 1, 0, 5, 1, 0, 1],\n",
      "[0, 1, 1, 5, 1, 0, 1],]\n",
      "\n",
      "Example 3: Output\n",
      "[\n",
      "[0, 0, 0],\n",
      "[2, 0, 0],\n",
      "[0, 0, 2],]\n",
      "\n",
      "Test\n",
      "[\n",
      "[1, 0, 1, 5, 1, 0, 1]\n",
      "[0, 1, 0, 5, 1, 0, 1]\n",
      "[1, 0, 1, 5, 0, 1, 0]]\n",
      "\n",
      "Your Response:\n"
     ]
    }
   ],
   "source": [
    "# an example of how the function works\n",
    "task_string = json_task_to_string(challenges, '0520fde7', 0)\n",
    "print (task_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a json output parser to parse the output, since LLMs aren't perfect at generating valid json\n",
    "# Defining a prediction as a list of lists\n",
    "class ARCPrediction(BaseModel):\n",
    "    prediction: List[List] = Field(..., description=\"A prediction for a task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating function to get task prediction, make prompt, make API calls to model and parse output with retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_prediction(challenge_tasks, task_id, test_input_index) -> List[List]:\n",
    "    \"\"\"\n",
    "    challenge_tasks: dict a list of tasks\n",
    "    task_id: str the id of the task we want to get a prediction for\n",
    "    test_input_index: the index of your test input. 96% of tests only have 1 input.\n",
    "\n",
    "    Given a task, predict the test output\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the string representation of your task\n",
    "    task_string = json_task_to_string(challenge_tasks, task_id, test_input_index)\n",
    "    \n",
    "    # Set up a parser to inject instructions into the prompt template.\n",
    "    parser = JsonOutputParser(pydantic_object=ARCPrediction)\n",
    "\n",
    "    # Create your prompt template. This is very rudimentary! You should edit this to do much better.\n",
    "    # For example, we don't tell the model what it's first attempt was (so it can do a different one), that might help!\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"You are a bot that is very good at solving puzzles. Below is a list of input and output pairs with a pattern.\" \n",
    "                    \"Identify the pattern, then apply that pattern to the test input to give a final output\"\n",
    "                    \"Just give valid json list of lists response back, nothing else. Do not explain your thoughts.\"\n",
    "                    \"{format_instructions}\\n{task_string}\\n\",\n",
    "        input_variables=[\"task_string\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Wrap up your chain with LCEL\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    # Optional, print out the prompt if you want to see it. If you use LangSmith you could view this there as well.\n",
    "    # print (f\"Prompt:\\n\\n{prompt.format(task_string=task_string)}\")\n",
    "    \n",
    "    # Finally, go get your prediction from your LLM. Ths will make the API call.\n",
    "    output = chain.invoke({\"task_string\": task_string})\n",
    "\n",
    "    # Because the output is structured, get the prediction key. If it isn't there, then just get the output\n",
    "    if isinstance(output, dict):\n",
    "        prediction = output.get('prediction', output)\n",
    "    else:\n",
    "        prediction = output\n",
    "\n",
    "    # Safety measure to error out if you don't get a list of lists of ints back. This will spark a retry later.\n",
    "    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in prediction):\n",
    "        print(\"Warning: Output must be a list of lists of integers.\")\n",
    "        print (f\"Errored Output: {prediction}\")\n",
    "        raise ValueError(\"Output must be a list of lists of integers.\")\n",
    "    \n",
    "    # Let's find the shape of our prediction\n",
    "    num_rows = len(prediction)\n",
    "    num_cols = len(prediction[0]) if num_rows > 0 else 0\n",
    "    print(f\"    Prediction Grid Size: {num_rows}x{num_cols}\\n\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(challenges, NUM_ATTEMPTS=2, RETRY_ATTEMPTS=3, NUM_TASKS=None):\n",
    "    \"\"\"\n",
    "    challenges: dict a list of challenges. This should come directly from your _challenges file\n",
    "    NUM_ATTEMPTS: int the number of times to attempt a prediction. The official competition has 2 attempts.\n",
    "    RETRY_ATTEMPTS: int the number of times to retry a prediction if it fails\n",
    "    NUM_TASKS: int, If set, this represents the the number of tasks you'd like to test. If None then the all challeneges will be tested\n",
    "\n",
    "    Loop through your challenges and produce a submission.json file you can submit for a score.\n",
    "    \"\"\"\n",
    "\n",
    "    # A dict to hold your submissions that you'll return after all predictions are made\n",
    "    submission = {}\n",
    "\n",
    "    # Run through each task in your challenge set\n",
    "    for i, task_id in enumerate(challenges):\n",
    "        task_attempts = []  # List to store all attempts for the current task\n",
    "\n",
    "        # Go through each test pair to get a prediction. 96% of challenges have 1 pair.\n",
    "        for t, pair in enumerate(challenges[task_id]['test']):\n",
    "            print(f\"Starting task #{i + 1} ({task_id}), pair #{t+1}\")\n",
    "\n",
    "            # Dictionary to store attempts for the current test pair\n",
    "            pair_attempts = {}  \n",
    "\n",
    "            # Run through each prediction attempt\n",
    "            for attempt in range(1, NUM_ATTEMPTS + 1):\n",
    "                attempt_key = f\"attempt_{attempt}\"\n",
    "                pair_attempts[attempt_key] = [] # Init your attempt\n",
    "\n",
    "                # Try to get a prediction, with retries in case of failure\n",
    "                for retry in range(RETRY_ATTEMPTS):\n",
    "                    try:\n",
    "                        print(f\"    Predicting attempt #{attempt}, retry #{retry + 1}\")\n",
    "                        prediction = get_task_prediction(challenge_tasks=challenges,\n",
    "                                                         task_id=task_id,\n",
    "                                                         test_input_index=t)\n",
    "                        \n",
    "                        # If you get a valid prediction (list of lists of ints) with no error, then log the attempt\n",
    "                        pair_attempts[attempt_key] = prediction\n",
    "                        break  # Break the retry loop if prediction is successful\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retrying: {e}\")\n",
    "                        if retry == RETRY_ATTEMPTS - 1:\n",
    "                            pair_attempts[attempt_key] = []  # Assign None if all retries fail\n",
    "\n",
    "            # After you get your attempts, append them to the task attempts\n",
    "            task_attempts.append(pair_attempts)\n",
    "\n",
    "        # Append the task attempts to the submission with the task_id as the key\n",
    "        submission[task_id] = task_attempts\n",
    "\n",
    "        # If you want to stop after N tasks, uncomment the below\n",
    "        if NUM_TASKS is not None and i + 1 == NUM_TASKS:\n",
    "            break\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating submission files and comparing it with solutions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "def create_submission_file(submission, file_name='submission.json'):\n",
    "    \"\"\"\n",
    "    Save a submission file to the specified file name\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        json.dump(submission, file)\n",
    "\n",
    "    print (f\"Submission saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to compare submission with solutions\n",
    "def score_submission(submission_file_name, solutions) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    submission_file_name: str, the file name of your submission file\n",
    "    solutions: dict, the ground truth solutions you'd like to test against\n",
    "    \n",
    "    Read a submission from file, score it, then return the score\n",
    "    \"\"\"\n",
    "    print (f\"Scoring {submission_file_name}\\n\")\n",
    "\n",
    "    # Open your submission file\n",
    "    with open(submission_file_name, \"r\") as file:\n",
    "        submission = json.load(file)\n",
    "\n",
    "    total_score = 0\n",
    "    total_tasks = 0\n",
    "\n",
    "    # Loop through each task in your submission to grade it\n",
    "    for task_id, task_submission in submission.items():\n",
    "        total_tasks += 1\n",
    "        task_score = 0\n",
    "        num_pairs = len(task_submission)\n",
    "\n",
    "        # Go through each task. Most will only have 1\n",
    "        for pair_index, pair_attempts in enumerate(task_submission):\n",
    "            print(f\"Scoring Task {task_id} pair #{pair_index+1}\")\n",
    "            pair_correct = False\n",
    "\n",
    "            # Look at both of your attempts\n",
    "            for attempt_key, attempt in pair_attempts.items():\n",
    "                \n",
    "                # check to see if one is correct\n",
    "                if attempt == solutions[task_id][pair_index]:\n",
    "                    print(f\"Task Id {task_id} pair {pair_index+1} {attempt_key} matches solution\")\n",
    "                    pair_correct = True\n",
    "                    break # If it is correct, log it and break the loop\n",
    "\n",
    "            if pair_correct:\n",
    "                task_score += 1\n",
    "\n",
    "        task_score /= num_pairs\n",
    "        total_score += task_score\n",
    "\n",
    "    return {\n",
    "        'total_score': total_score,\n",
    "        'total_tasks_scored': total_tasks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main function to bring everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(task_set='training', NUM_TASKS=None, submission_file_name='submission_finetune4o.json'):\n",
    "    # Load datasets\n",
    "    challenges, solutions = load_tasks_from_file(task_set=task_sets[task_set])\n",
    "\n",
    "    # # Run the model\n",
    "    submission = run_model(challenges, NUM_TASKS=NUM_TASKS)\n",
    "\n",
    "    # Create (and overwrite) a submission file\n",
    "    create_submission_file(submission, file_name=submission_file_name)\n",
    "\n",
    "    # Score the submission\n",
    "    score_result = score_submission(solutions = solutions, submission_file_name=submission_file_name)\n",
    "\n",
    "    print(f\"Final score: {score_result['total_score']} of {score_result['total_tasks_scored']} ({round(score_result['total_score']/score_result['total_tasks_scored'] * 100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task #1 (00576224), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 6x6\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 6x6\n",
      "\n",
      "Starting task #2 (009d5c81), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 14x14\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 14x14\n",
      "\n",
      "Starting task #3 (00dbd492), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 24x20\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 18x20\n",
      "\n",
      "Starting task #4 (03560426), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 10x10\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 10x10\n",
      "\n",
      "Starting task #5 (05a7bcf2), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 36x30\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 26x28\n",
      "\n",
      "Starting task #6 (0607ce86), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 27x22\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 24x22\n",
      "\n",
      "Starting task #7 (0692e18c), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Output must be a list of lists of integers.\n",
      "Errored Output: [[[0, 0, 0, 3, 0, 0, 0, 0, 0], [0, 0, 3, 3, 0, 0, 0, 0, 0], [0, 3, 3, 0, 0, 0, 0, 0, 0], [0, 3, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 3, 3, 0, 0, 0], [0, 0, 0, 0, 0, 3, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 3], [0, 0, 0, 0, 0, 0, 3, 0, 3], [0, 0, 0, 0, 0, 0, 0, 3, 0]]]\n",
      "Retrying: Output must be a list of lists of integers.\n",
      "    Predicting attempt #1, retry #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 9x9\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 9x9\n",
      "\n",
      "Starting task #8 (070dd51e), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 20x20\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 21x20\n",
      "\n",
      "Starting task #9 (08573cc6), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 13x13\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 14x13\n",
      "\n",
      "Starting task #10 (0934a4d8), pair #1\n",
      "    Predicting attempt #1, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Output must be a list of lists of integers.\n",
      "Errored Output: [[[6, 6, 5, 1], [4, 4, 7, 7], [7, 7, 7, 2]], [[2, 2, 6, 4], [6, 2, 2, 2], [2, 6, 6, 6]], [[7, 7, 7, 4], [4, 4, 1, 5], [4, 4, 5, 1], [2, 6, 4, 6]]]\n",
      "Retrying: Output must be a list of lists of integers.\n",
      "    Predicting attempt #1, retry #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 4x3\n",
      "\n",
      "    Predicting attempt #2, retry #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=Ellipsis description='A prediction for a task' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction Grid Size: 5x6\n",
      "\n",
      "Submission saved to submission.json\n",
      "Scoring submission.json\n",
      "\n",
      "Scoring Task 00576224 pair #1\n",
      "Task Id 00576224 pair 1 attempt_1 matches solution\n",
      "Scoring Task 009d5c81 pair #1\n",
      "Scoring Task 00dbd492 pair #1\n",
      "Scoring Task 03560426 pair #1\n",
      "Scoring Task 05a7bcf2 pair #1\n",
      "Scoring Task 0607ce86 pair #1\n",
      "Scoring Task 0692e18c pair #1\n",
      "Scoring Task 070dd51e pair #1\n",
      "Scoring Task 08573cc6 pair #1\n",
      "Scoring Task 0934a4d8 pair #1\n",
      "Final score: 1.0 of 10 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "main(task_set='evaluation', NUM_TASKS=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
