{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from glob import glob\n",
    "import langchain  # Main LangChain import\n",
    "from langchain.chat_models import ChatOpenAI  # Updated import path for ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate  # Updated import path\n",
    "from langchain.schema import AIMessage  # Updated import path\n",
    "from pydantic import BaseModel, Field  # Use pydantic directly\n",
    "from typing import List, Tuple, Dict\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Configure Logging\n",
    "# ==========================\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger('ReWOO_LangGraph')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create handlers\n",
    "c_handler = logging.StreamHandler(sys.stdout)\n",
    "f_handler = logging.FileHandler('rewoolanggraph.log', mode='w')\n",
    "c_handler.setLevel(logging.INFO)\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add to handlers\n",
    "c_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "c_handler.setFormatter(c_format)\n",
    "f_handler.setFormatter(f_format)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(c_handler)\n",
    "logger.addHandler(f_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Function to load JSON files\n",
    "# ==========================\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Loading Files\n",
    "# ==========================\n",
    "\n",
    "base_path = 'arc-agi-genesis/data/challenges/'\n",
    "training_challenges = load_json(base_path + 'arc-agi_training_challenges.json')\n",
    "training_solutions = load_json(base_path + 'arc-agi_training_solutions.json')\n",
    "\n",
    "evaluation_challenges = load_json(base_path + 'arc-agi_evaluation_challenges.json')\n",
    "evaluation_solutions = load_json(base_path + 'arc-agi_evaluation_solutions.json')\n",
    "\n",
    "test_challenges = load_json(base_path + 'arc-agi_test_challenges.json')\n",
    "\n",
    "task_sets = {\n",
    "    'training': {\n",
    "        'challenges': training_challenges,\n",
    "        'solutions': training_solutions,\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'challenges': evaluation_challenges,\n",
    "        'solutions': evaluation_solutions,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Function to load tasks from a pre-loaded task set\n",
    "# ==========================\n",
    "\n",
    "def load_tasks_from_file(task_set):\n",
    "    \"\"\"\n",
    "    Loads the tasks from the pre-loaded JSON data and returns the challenges and solutions tasks.\n",
    "    \"\"\"\n",
    "    challenges = task_set['challenges']\n",
    "    solutions = task_set['solutions']\n",
    "\n",
    "    return challenges, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training challenges = 400\n",
      "Number of solutions of training challenges = 400\n",
      "{\n",
      "  \"test\": [\n",
      "    {\n",
      "      \"input\": [\n",
      "        [\n",
      "          1,\n",
      "          0,\n",
      "          1,\n",
      "          5,\n",
      "          1,\n",
      "          0,\n",
      "          1\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          1,\n",
      "          0,\n",
      "          5,\n",
      "          1,\n",
      "          0,\n",
      "          1\n",
      "        ],\n",
      "        [\n",
      "          1,\n",
      "          0,\n",
      "          1,\n",
      "          5,\n",
      "          0,\n",
      "          1,\n",
      "          0\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"train\": [\n",
      "    {\n",
      "      \"input\": [\n",
      "        [\n",
      "          1,\n",
      "          0,\n",
      "          0,\n",
      "          5,\n",
      "          0,\n",
      "          1,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          1,\n",
      "          0,\n",
      "          5,\n",
      "          1,\n",
      "          1,\n",
      "          1\n",
      "        ],\n",
      "        [\n",
      "          1,\n",
      "          0,\n",
      "          0,\n",
      "          5,\n",
      "          0,\n",
      "          0,\n",
      "          0\n",
      "        ]\n",
      "      ],\n",
      "      \"output\": [\n",
      "        [\n",
      "          0,\n",
      "          0,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          2,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          0,\n",
      "          0\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"input\": [\n",
      "        [\n",
      "          1,\n",
      "          1,\n",
      "          0,\n",
      "          5,\n",
      "          0,\n",
      "          1,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          0,\n",
      "          1,\n",
      "          5,\n",
      "          1,\n",
      "          1,\n",
      "          1\n",
      "        ],\n",
      "        [\n",
      "          1,\n",
      "          1,\n",
      "          0,\n",
      "          5,\n",
      "          0,\n",
      "          1,\n",
      "          0\n",
      "        ]\n",
      "      ],\n",
      "      \"output\": [\n",
      "        [\n",
      "          0,\n",
      "          2,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          0,\n",
      "          2\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          2,\n",
      "          0\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"input\": [\n",
      "        [\n",
      "          0,\n",
      "          0,\n",
      "          1,\n",
      "          5,\n",
      "          0,\n",
      "          0,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          1,\n",
      "          1,\n",
      "          0,\n",
      "          5,\n",
      "          1,\n",
      "          0,\n",
      "          1\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          1,\n",
      "          1,\n",
      "          5,\n",
      "          1,\n",
      "          0,\n",
      "          1\n",
      "        ]\n",
      "      ],\n",
      "      \"output\": [\n",
      "        [\n",
      "          0,\n",
      "          0,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          2,\n",
      "          0,\n",
      "          0\n",
      "        ],\n",
      "        [\n",
      "          0,\n",
      "          0,\n",
      "          2\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Print Dataset Information\n",
    "# ==========================\n",
    "\n",
    "print(f'Number of training challenges = {len(training_challenges)}')\n",
    "print(f'Number of solutions of training challenges = {len(training_solutions)}')\n",
    "# Loading tasks from the 'training' task set\n",
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "print(json.dumps(challenges['0520fde7'], indent=2))  # Accessing a specific challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\au725081\\AppData\\Local\\Temp\\ipykernel_23452\\568330537.py:14: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model='gpt-4o-mini', openai_api_key=openai_api_key, max_tokens=3000)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Initializing LLM Client to Use\n",
    "# ==========================\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('api.env')\n",
    "\n",
    "# Get the OpenAI API key from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the ChatOpenAI model with the API key\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', openai_api_key=openai_api_key, max_tokens=3000)\n",
    "\n",
    "## And in case you want to try Anthropic\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# llm = ChatAnthropic(model='claude-3-5-sonnet-20240620', api_key=os.getenv(\"ANTHROPIC_API_KEY\"), max_tokens=3000)\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"), max_tokens=3000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Examples\n",
      "Example 1: Input\n",
      "[\n",
      "[8, 6],\n",
      "[6, 4],]\n",
      "\n",
      "Example 1: Output\n",
      "[\n",
      "[8, 6, 8, 6, 8, 6],\n",
      "[6, 4, 6, 4, 6, 4],\n",
      "[6, 8, 6, 8, 6, 8],\n",
      "[4, 6, 4, 6, 4, 6],\n",
      "[8, 6, 8, 6, 8, 6],\n",
      "[6, 4, 6, 4, 6, 4],]\n",
      "\n",
      "Test\n",
      "[\n",
      "[3, 2]\n",
      "[7, 8]]\n",
      "\n",
      "Your Response:\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Converting Train and Test Pairs to a String Format Ideal for LLMs\n",
    "# ==========================\n",
    "\n",
    "def json_task_to_string(challenge_tasks: dict, task_id: str, test_input_index: int) -> str:\n",
    "    \"\"\"\n",
    "    challenge_tasks: dict a list of tasks\n",
    "    task_id: str the id of the task we want to convert to a string\n",
    "\n",
    "    Convert your json task into a string so you can pass it to your LLM.\n",
    "    This is a crucial step where you can use your creativity to edit how tasks are represented.\n",
    "    \"\"\"\n",
    "    json_task = challenge_tasks[task_id]\n",
    "\n",
    "    final_output = \"\"\n",
    "\n",
    "    train_tasks = json_task['train']\n",
    "    test_task = json_task['test']\n",
    "\n",
    "    final_output = \"Training Examples\\n\"\n",
    "\n",
    "    for i, task in enumerate(train_tasks):\n",
    "        final_output += f\"Example {i + 1}: Input\\n[\"\n",
    "        for row in task['input']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "        final_output += f\"Example {i + 1}: Output\\n[\"\n",
    "\n",
    "        for row in task['output']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "\n",
    "    final_output += \"Test\\n[\"\n",
    "    for row in test_task[test_input_index]['input']:\n",
    "        final_output += f\"\\n{str(row)}\"\n",
    "\n",
    "    final_output += \"]\\n\\nYour Response:\"\n",
    "\n",
    "    return final_output\n",
    "\n",
    "# Example of how the function works\n",
    "\n",
    "# Wrap the example_task in a task_id\n",
    "example_tasks = {\n",
    "    'example_id': example_task\n",
    "}\n",
    "\n",
    "# Convert to string using the correct task_id\n",
    "task_string = json_task_to_string(example_tasks, 'example_id', 0)\n",
    "print(task_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Using a Structured Output Parser to Parse the Output\n",
    "# ==========================\n",
    "\n",
    "# Defining a prediction as a list of lists\n",
    "class ARCPrediction(BaseModel):\n",
    "    prediction: List[List[int]] = Field(..., description=\"A prediction for a task\")\n",
    "\n",
    "# Define the planner output model\n",
    "class PlannerStep(BaseModel):\n",
    "    plan: str\n",
    "    step_id: str\n",
    "    tool: str\n",
    "    tool_input: str\n",
    "\n",
    "class PlannerOutput(BaseModel):\n",
    "    steps: List[PlannerStep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructuredOutputParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 48\u001b[0m\n\u001b[0;32m     21\u001b[0m planner_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFor the following task, create a detailed step-by-step plan to solve the problem. For each step, specify the tool to use and the exact input for that tool. Each step should follow the format: \u001b[39m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124mPlan: <Detailed plan description>\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124mBegin!\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124mDescribe your plans with rich details. Each Plan should be followed by only one #E.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize the structured output parser for the planner\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m planner_parser \u001b[38;5;241m=\u001b[39m \u001b[43mStructuredOutputParser\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pydantic(PlannerOutput)\n\u001b[0;32m     49\u001b[0m planner_format_instructions \u001b[38;5;241m=\u001b[39m planner_parser\u001b[38;5;241m.\u001b[39mget_format_instructions()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Update the planner prompt to include format instructions\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StructuredOutputParser' is not defined"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Setting Up LangGraph for ReWOO Framework\n",
    "# ==========================\n",
    "\n",
    "# Define the ReWOO State\n",
    "class ReWOO(TypedDict):\n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List[Tuple[str, str, str, str]]\n",
    "    results: dict\n",
    "    result: List[List[int]]  # Changed from 'str' to 'List[List[int]]'\n",
    "\n",
    "# Initialize the StateGraph\n",
    "graph_builder = StateGraph(ReWOO)\n",
    "\n",
    "# Define the Planner Node\n",
    "import re\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the planner prompt with structured output instructions\n",
    "planner_prompt = \"\"\"For the following task, create a detailed step-by-step plan to solve the problem. For each step, specify the tool to use and the exact input for that tool. Each step should follow the format: \n",
    "\n",
    "Plan: <Detailed plan description>\n",
    "<Step Identifier> = <Tool>[<Tool Input>]\n",
    "\n",
    "Only use the tools provided below:\n",
    "\n",
    "1. **LLM[input]**: A pretrained language model like yourself. Use this when you need to leverage general world knowledge or perform reasoning. The input can be any instruction or question.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Task: Calculate the number of hours Rebecca worked if Thomas worked x hours, Toby worked 10 hours less than twice what Thomas worked, and Rebecca worked 8 hours less than Toby.\n",
    "\n",
    "Plan: \n",
    "Plan: Translate the problem into algebraic equations.\n",
    "#E1 = LLM[Translate the problem into algebraic equations based on the given information.]\n",
    "Plan: Solve for Thomas's hours.\n",
    "#E2 = LLM[Solve the equation derived from #E1 to find the value of x.]\n",
    "Plan: Calculate Toby's hours using the value of x from #E2.\n",
    "#E3 = LLM[Calculate Toby's hours as 2*x - 10 using the value of x obtained from #E2.]\n",
    "Plan: Determine Rebecca's hours based on Toby's hours from #E3.\n",
    "#E4 = LLM[Calculate Rebecca's hours as #E3 - 8.]\n",
    "\n",
    "Begin!\n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\"\"\"\n",
    "\n",
    "# Initialize the structured output parser for the planner\n",
    "planner_parser = StructuredOutputParser.from_pydantic(PlannerOutput)\n",
    "planner_format_instructions = planner_parser.get_format_instructions()\n",
    "\n",
    "# Update the planner prompt to include format instructions\n",
    "planner_prompt_full = f\"\"\"For the following task, create a detailed step-by-step plan to solve the problem. For each step, specify the tool to use and the exact input for that tool. Each step should follow the format: \n",
    "\n",
    "Plan: <Detailed plan description>\n",
    "<Step Identifier> = <Tool>[<Tool Input>]\n",
    "\n",
    "Only use the tools provided below:\n",
    "\n",
    "1. **LLM[input]**: A pretrained language model like yourself. Use this when you need to leverage general world knowledge or perform reasoning. The input can be any instruction or question.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Task: Calculate the number of hours Rebecca worked if Thomas worked x hours, Toby worked 10 hours less than twice what Thomas worked, and Rebecca worked 8 hours less than Toby.\n",
    "\n",
    "Plan: \n",
    "Plan: Translate the problem into algebraic equations.\n",
    "#E1 = LLM[Translate the problem into algebraic equations based on the given information.]\n",
    "Plan: Solve for Thomas's hours.\n",
    "#E2 = LLM[Solve the equation derived from #E1 to find the value of x.]\n",
    "Plan: Calculate Toby's hours using the value of x from #E2.\n",
    "#E3 = LLM[Calculate Toby's hours as 2*x - 10 using the value of x obtained from #E2.]\n",
    "Plan: Determine Rebecca's hours based on Toby's hours from #E3.\n",
    "#E4 = LLM[Calculate Rebecca's hours as #E3 - 8.]\n",
    "\n",
    "Begin!\n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\n",
    "\n",
    "{planner_format_instructions}\n",
    "\n",
    "Task: {{task}}\"\"\"\n",
    "\n",
    "planner_chat_prompt = ChatPromptTemplate.from_messages([(\"user\", planner_prompt_full)])\n",
    "\n",
    "# Define the planner with the parser\n",
    "planner = (planner_chat_prompt | llm | planner_parser)\n",
    "\n",
    "def get_plan(state: ReWOO):\n",
    "    task = state[\"task\"]\n",
    "    logger.debug(f\"Planner Node: Generating plan for task: {task}\")\n",
    "    result = planner.invoke({\"task\": task})\n",
    "    logger.debug(f\"Planner Node: Received plan: {result}\")\n",
    "    \n",
    "    # Extract the steps from the structured output\n",
    "    steps = [(step.plan, step.step_id, step.tool, step.tool_input) for step in result.steps]\n",
    "    logger.debug(f\"Planner Node: Extracted steps: {steps}\")\n",
    "    \n",
    "    return {\"steps\": steps, \"plan_string\": result.json()}\n",
    "\n",
    "# Define the Tool Execution Node (using only LLM, no Google search)\n",
    "def _get_current_task(state: ReWOO):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1\n",
    "\n",
    "def tool_execution(state: ReWOO):\n",
    "    \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        logger.debug(\"Tool Execution Node: No more steps to execute.\")\n",
    "        return {}  # No more steps to execute\n",
    "\n",
    "    step = state[\"steps\"][_step - 1]\n",
    "    _plan, step_name, tool, tool_input = step\n",
    "    logger.debug(f\"Tool Execution Node: Executing Step {_step}: {step_name} = {tool}[{tool_input}]\")\n",
    "\n",
    "    _results = state.get(\"results\", {})\n",
    "    # Substitute any variables in the tool_input\n",
    "    for k, v in _results.items():\n",
    "        tool_input = tool_input.replace(k, v)\n",
    "\n",
    "    try:\n",
    "        if tool.lower() == \"llm\":\n",
    "            result = llm.invoke(tool_input)\n",
    "            logger.debug(f\"Tool Execution Node: LLM response: {result.content}\")\n",
    "            # Parse the LLM's JSON response\n",
    "            parsed_result = json.loads(result.content.strip())\n",
    "            response = parsed_result.get(\"prediction\", [])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tool: {tool}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Tool Execution Node: Error executing tool {tool}: {e}\")\n",
    "        response = f\"Error: {e}\"\n",
    "\n",
    "    _results[step_name] = response  # Assign the parsed response directly\n",
    "    logger.debug(f\"Tool Execution Node: Updated results: {_results}\")\n",
    "\n",
    "    return {\"results\": _results}\n",
    "\n",
    "# Define the Solver Node\n",
    "solve_prompt = \"\"\"Solve the following task or problem. To solve the problem, we have made step-by-step Plan and retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to provided Evidence above. Respond with the answer directly in valid JSON format without any extra words.\n",
    "\n",
    "Task: {task}\n",
    "Response:\"\"\"\n",
    "\n",
    "def solve(state: ReWOO):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, tool, tool_input in state[\"steps\"]:\n",
    "        _results = state.get(\"results\", {})\n",
    "        for k, v in _results.items():\n",
    "            tool_input = tool_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {tool}[{tool_input}]\\n\"\n",
    "\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    logger.debug(f\"Solver Node: Generating final response with prompt:\\n{prompt}\")\n",
    "    result = llm.invoke(prompt)\n",
    "    logger.debug(f\"Solver Node: Solver response: {result.content}\")\n",
    "    \n",
    "    try:\n",
    "        parsed_result = json.loads(result.content.strip())\n",
    "        prediction = parsed_result.get(\"prediction\", [])\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"JSON parsing error: {e}\")\n",
    "        prediction = []\n",
    "\n",
    "    return {\"result\": prediction}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Define Routing Function\n",
    "# ==========================\n",
    "\n",
    "def _route(state: ReWOO):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        logger.debug(\"Routing Function: All tasks executed, routing to 'solve' node.\")\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        logger.debug(f\"Routing Function: Continuing with tool execution node.\")\n",
    "        return \"tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Building the Graph\n",
    "# ==========================\n",
    "\n",
    "graph_builder.add_node(\"plan\", get_plan)\n",
    "graph_builder.add_node(\"tool\", tool_execution)\n",
    "graph_builder.add_node(\"solve\", solve)\n",
    "\n",
    "graph_builder.add_edge(\"plan\", \"tool\")\n",
    "graph_builder.add_conditional_edges(\"tool\", _route)\n",
    "graph_builder.add_edge(\"solve\", END)\n",
    "graph_builder.add_edge(START, \"plan\")\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Creating Function to Get Task Prediction\n",
    "# ==========================\n",
    "\n",
    "def get_task_prediction(challenge_tasks, task_id, test_input_index) -> List[List[int]]:\n",
    "    task_string = json_task_to_string(challenge_tasks, task_id, test_input_index)\n",
    "    task_data = challenge_tasks[task_id]\n",
    "\n",
    "    initial_state: ReWOO = {\n",
    "        'task': task_string,\n",
    "        'plan_string': '',\n",
    "        'steps': [],\n",
    "        'results': {},\n",
    "        'result': []\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Starting task prediction for task_id: {task_id}, test_input_index: {test_input_index}\")\n",
    "    logger.debug(f\"Initial State: {initial_state}\")\n",
    "\n",
    "    final_state = graph.invoke(initial_state)\n",
    "    logger.debug(f\"Final State after graph execution: {final_state}\")\n",
    "\n",
    "    prediction = final_state.get('result', [])\n",
    "\n",
    "    # Safety check\n",
    "    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in prediction):\n",
    "        logger.warning(\"Output must be a list of lists of integers.\")\n",
    "        logger.warning(f\"Errored Output: {prediction}\")\n",
    "        raise ValueError(\"Output must be a list of lists of integers.\")\n",
    "\n",
    "    num_rows = len(prediction)\n",
    "    num_cols = len(prediction[0]) if num_rows > 0 else 0\n",
    "    logger.info(f\"Prediction Grid Size: {num_rows}x{num_cols}\")\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Run Model Function\n",
    "# ==========================\n",
    "\n",
    "def run_model(challenges, solutions, NUM_ATTEMPTS=2, RETRY_ATTEMPTS=3, NUM_TASKS=None):\n",
    "    \"\"\"\n",
    "    challenges: dict a list of challenges. This should come directly from your _challenges file\n",
    "    solutions: dict, the ground truth solutions you'd like to test against\n",
    "    NUM_ATTEMPTS: int the number of times to attempt a prediction. The official competition has 2 attempts.\n",
    "    RETRY_ATTEMPTS: int the number of times to retry a prediction if it fails\n",
    "    NUM_TASKS: int, If set, this represents the the number of tasks you'd like to test. If None then the all challenges will be tested\n",
    "\n",
    "    Loop through your challenges and produce a submission.json file you can submit for a score.\n",
    "    \"\"\"\n",
    "\n",
    "    # A dict to hold your submissions that you'll return after all predictions are made\n",
    "    submission = {}\n",
    "\n",
    "    # Run through each task in your challenge set\n",
    "    for i, task_id in enumerate(challenges):\n",
    "        task_attempts = []  # List to store all attempts for the current task\n",
    "\n",
    "        # Go through each test pair to get a prediction. 96% of challenges have 1 pair.\n",
    "        for t, pair in enumerate(challenges[task_id]['test']):\n",
    "            logger.info(f\"Starting task #{i + 1} ({task_id}), pair #{t+1}\")\n",
    "\n",
    "            # Dictionary to store attempts for the current test pair\n",
    "            pair_attempts = {}\n",
    "\n",
    "            # Run through each prediction attempt\n",
    "            for attempt in range(1, NUM_ATTEMPTS + 1):\n",
    "                attempt_key = f\"attempt_{attempt}\"\n",
    "                pair_attempts[attempt_key] = []  # Init your attempt\n",
    "\n",
    "                # Run through retries\n",
    "                for retry in range(RETRY_ATTEMPTS):\n",
    "                    try:\n",
    "                        logger.info(f\"    Predicting attempt #{attempt}, retry #{retry + 1}\")\n",
    "                        prediction = get_task_prediction(challenge_tasks=challenges,\n",
    "                                                         task_id=task_id,\n",
    "                                                         test_input_index=t)\n",
    "\n",
    "                        # If you get a valid prediction (list of lists of ints) with no error, then log the attempt\n",
    "                        pair_attempts[attempt_key] = prediction\n",
    "                        logger.debug(f\"    Prediction successful: {prediction}\")\n",
    "                        break  # Break the retry loop if prediction is successful\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"    Retrying: {e}\")\n",
    "                        if retry == RETRY_ATTEMPTS - 1:\n",
    "                            pair_attempts[attempt_key] = []  # Assign empty list if all retries fail\n",
    "                            logger.error(f\"    All retries failed for attempt #{attempt}\")\n",
    "\n",
    "            # After you get your attempts, append them to the task attempts\n",
    "            task_attempts.append(pair_attempts)\n",
    "\n",
    "        # Append the task attempts to the submission with the task_id as the key\n",
    "        submission[task_id] = task_attempts\n",
    "\n",
    "        # If you want to stop after N tasks, uncomment the below\n",
    "        if NUM_TASKS is not None and i + 1 == NUM_TASKS:\n",
    "            logger.info(f\"Reached the limit of {NUM_TASKS} tasks.\")\n",
    "            break\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Creating Submission Files and Comparing with Solutions\n",
    "# ==========================\n",
    "\n",
    "# Create submission file\n",
    "def create_submission_file(submission, file_name='submission.json'):\n",
    "    \"\"\"\n",
    "    Save a submission file to the specified file name\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        json.dump(submission, file, indent=2)\n",
    "\n",
    "    logger.info(f\"Submission saved to {file_name}\")\n",
    "\n",
    "# Create function to compare submission with solutions\n",
    "def score_submission(submission_file_name, solutions) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    submission_file_name: str, the file name of your submission file\n",
    "    solutions: dict, the ground truth solutions you'd like to test against\n",
    "\n",
    "    Read a submission from file, score it, then return the score\n",
    "    \"\"\"\n",
    "    logger.info(f\"Scoring {submission_file_name}\\n\")\n",
    "\n",
    "    # Open your submission file\n",
    "    with open(submission_file_name, \"r\") as file:\n",
    "        submission = json.load(file)\n",
    "\n",
    "    total_score = 0\n",
    "    total_tasks = 0\n",
    "\n",
    "    # Loop through each task in your submission to grade it\n",
    "    for task_id, task_submission in submission.items():\n",
    "        total_tasks += 1\n",
    "        task_score = 0\n",
    "        num_pairs = len(task_submission)\n",
    "\n",
    "        # Go through each task. Most will only have 1\n",
    "        for pair_index, pair_attempts in enumerate(task_submission):\n",
    "            logger.info(f\"Scoring Task {task_id} pair #{pair_index+1}\")\n",
    "            pair_correct = False\n",
    "\n",
    "            # Look at both of your attempts\n",
    "            for attempt_key, attempt in pair_attempts.items():\n",
    "                # check to see if one is correct\n",
    "                if attempt == solutions[task_id][pair_index]:\n",
    "                    logger.info(f\"Task Id {task_id} pair {pair_index+1} {attempt_key} matches solution\")\n",
    "                    pair_correct = True\n",
    "                    break  # If it is correct, log it and break the loop\n",
    "\n",
    "            if pair_correct:\n",
    "                task_score += 1\n",
    "                logger.debug(f\"Task {task_id} pair {pair_index+1} scored 1 point.\")\n",
    "            else:\n",
    "                logger.debug(f\"Task {task_id} pair {pair_index+1} scored 0 points.\")\n",
    "\n",
    "        task_score /= num_pairs\n",
    "        total_score += task_score\n",
    "\n",
    "    return {\n",
    "        'total_score': total_score,\n",
    "        'total_tasks_scored': total_tasks\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# The Main Function to Bring Everything Together\n",
    "# ==========================\n",
    "\n",
    "def main(task_set='training', NUM_TASKS=None, submission_file_name='submission.json'):\n",
    "    # Load datasets\n",
    "    challenges, solutions = load_tasks_from_file(task_set=task_sets[task_set])\n",
    "\n",
    "    # Run the model\n",
    "    submission = run_model(challenges, solutions, NUM_TASKS=NUM_TASKS)\n",
    "\n",
    "    # Create (and overwrite) a submission file\n",
    "    create_submission_file(submission, file_name=submission_file_name)\n",
    "\n",
    "    # Score the submission\n",
    "    score_result = score_submission(submission_file_name=submission_file_name, solutions=solutions)\n",
    "\n",
    "    final_percentage = (score_result['total_score'] / score_result['total_tasks_scored'] * 100) if score_result['total_tasks_scored'] > 0 else 0\n",
    "    logger.info(f\"Final score: {score_result['total_score']} of {score_result['total_tasks_scored']} ({round(final_percentage, 2)}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:11:53,207 - INFO - Starting task #1 (00576224), pair #1\n",
      "2024-10-16 11:11:53,207 - INFO - Starting task #1 (00576224), pair #1\n",
      "2024-10-16 11:11:53,207 - INFO - Starting task #1 (00576224), pair #1\n",
      "2024-10-16 11:11:53,207 - INFO - Starting task #1 (00576224), pair #1\n",
      "2024-10-16 11:11:53,207 - INFO - Starting task #1 (00576224), pair #1\n",
      "2024-10-16 11:11:53,214 - INFO -     Predicting attempt #1, retry #1\n",
      "2024-10-16 11:11:53,214 - INFO -     Predicting attempt #1, retry #1\n",
      "2024-10-16 11:11:53,214 - INFO -     Predicting attempt #1, retry #1\n",
      "2024-10-16 11:11:53,214 - INFO -     Predicting attempt #1, retry #1\n",
      "2024-10-16 11:11:53,214 - INFO -     Predicting attempt #1, retry #1\n",
      "2024-10-16 11:11:53,220 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:53,220 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:53,220 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:53,220 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:53,220 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:59,372 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:11:59,372 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:11:59,372 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:11:59,372 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:11:59,372 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:11:59,376 - INFO -     Predicting attempt #1, retry #2\n",
      "2024-10-16 11:11:59,376 - INFO -     Predicting attempt #1, retry #2\n",
      "2024-10-16 11:11:59,376 - INFO -     Predicting attempt #1, retry #2\n",
      "2024-10-16 11:11:59,376 - INFO -     Predicting attempt #1, retry #2\n",
      "2024-10-16 11:11:59,376 - INFO -     Predicting attempt #1, retry #2\n",
      "2024-10-16 11:11:59,378 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:59,378 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:59,378 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:59,378 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:11:59,378 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:05,983 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:05,983 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:05,983 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:05,983 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:05,983 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:05,988 - INFO -     Predicting attempt #1, retry #3\n",
      "2024-10-16 11:12:05,988 - INFO -     Predicting attempt #1, retry #3\n",
      "2024-10-16 11:12:05,988 - INFO -     Predicting attempt #1, retry #3\n",
      "2024-10-16 11:12:05,988 - INFO -     Predicting attempt #1, retry #3\n",
      "2024-10-16 11:12:05,988 - INFO -     Predicting attempt #1, retry #3\n",
      "2024-10-16 11:12:05,992 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:05,992 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:05,992 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:05,992 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:05,992 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:11,535 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:11,535 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:11,535 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:11,535 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:11,535 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:11,541 - ERROR -     All retries failed for attempt #1\n",
      "2024-10-16 11:12:11,541 - ERROR -     All retries failed for attempt #1\n",
      "2024-10-16 11:12:11,541 - ERROR -     All retries failed for attempt #1\n",
      "2024-10-16 11:12:11,541 - ERROR -     All retries failed for attempt #1\n",
      "2024-10-16 11:12:11,541 - ERROR -     All retries failed for attempt #1\n",
      "2024-10-16 11:12:11,544 - INFO -     Predicting attempt #2, retry #1\n",
      "2024-10-16 11:12:11,544 - INFO -     Predicting attempt #2, retry #1\n",
      "2024-10-16 11:12:11,544 - INFO -     Predicting attempt #2, retry #1\n",
      "2024-10-16 11:12:11,544 - INFO -     Predicting attempt #2, retry #1\n",
      "2024-10-16 11:12:11,544 - INFO -     Predicting attempt #2, retry #1\n",
      "2024-10-16 11:12:11,546 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:11,546 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:11,546 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:11,546 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:11,546 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:17,131 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:17,131 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:17,131 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:17,131 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:17,131 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:17,137 - INFO -     Predicting attempt #2, retry #2\n",
      "2024-10-16 11:12:17,137 - INFO -     Predicting attempt #2, retry #2\n",
      "2024-10-16 11:12:17,137 - INFO -     Predicting attempt #2, retry #2\n",
      "2024-10-16 11:12:17,137 - INFO -     Predicting attempt #2, retry #2\n",
      "2024-10-16 11:12:17,137 - INFO -     Predicting attempt #2, retry #2\n",
      "2024-10-16 11:12:17,140 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:17,140 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:17,140 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:17,140 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:17,140 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:23,247 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:23,247 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:23,247 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:23,247 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:23,247 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:23,252 - INFO -     Predicting attempt #2, retry #3\n",
      "2024-10-16 11:12:23,252 - INFO -     Predicting attempt #2, retry #3\n",
      "2024-10-16 11:12:23,252 - INFO -     Predicting attempt #2, retry #3\n",
      "2024-10-16 11:12:23,252 - INFO -     Predicting attempt #2, retry #3\n",
      "2024-10-16 11:12:23,252 - INFO -     Predicting attempt #2, retry #3\n",
      "2024-10-16 11:12:23,255 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:23,255 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:23,255 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:23,255 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:23,255 - INFO - Starting task prediction for task_id: 00576224, test_input_index: 0\n",
      "2024-10-16 11:12:28,008 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:28,008 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:28,008 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:28,008 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:28,008 - ERROR -     Retrying: Must write to at least one of ['task', 'plan_string', 'steps', 'results', 'result']\n",
      "2024-10-16 11:12:28,013 - ERROR -     All retries failed for attempt #2\n",
      "2024-10-16 11:12:28,013 - ERROR -     All retries failed for attempt #2\n",
      "2024-10-16 11:12:28,013 - ERROR -     All retries failed for attempt #2\n",
      "2024-10-16 11:12:28,013 - ERROR -     All retries failed for attempt #2\n",
      "2024-10-16 11:12:28,013 - ERROR -     All retries failed for attempt #2\n",
      "2024-10-16 11:12:28,015 - INFO - Reached the limit of 1 tasks.\n",
      "2024-10-16 11:12:28,015 - INFO - Reached the limit of 1 tasks.\n",
      "2024-10-16 11:12:28,015 - INFO - Reached the limit of 1 tasks.\n",
      "2024-10-16 11:12:28,015 - INFO - Reached the limit of 1 tasks.\n",
      "2024-10-16 11:12:28,015 - INFO - Reached the limit of 1 tasks.\n",
      "2024-10-16 11:12:28,019 - INFO - Submission saved to submission.json\n",
      "2024-10-16 11:12:28,019 - INFO - Submission saved to submission.json\n",
      "2024-10-16 11:12:28,019 - INFO - Submission saved to submission.json\n",
      "2024-10-16 11:12:28,019 - INFO - Submission saved to submission.json\n",
      "2024-10-16 11:12:28,019 - INFO - Submission saved to submission.json\n",
      "2024-10-16 11:12:28,024 - INFO - Scoring submission.json\n",
      "\n",
      "2024-10-16 11:12:28,024 - INFO - Scoring submission.json\n",
      "\n",
      "2024-10-16 11:12:28,024 - INFO - Scoring submission.json\n",
      "\n",
      "2024-10-16 11:12:28,024 - INFO - Scoring submission.json\n",
      "\n",
      "2024-10-16 11:12:28,024 - INFO - Scoring submission.json\n",
      "\n",
      "2024-10-16 11:12:28,029 - INFO - Scoring Task 00576224 pair #1\n",
      "2024-10-16 11:12:28,029 - INFO - Scoring Task 00576224 pair #1\n",
      "2024-10-16 11:12:28,029 - INFO - Scoring Task 00576224 pair #1\n",
      "2024-10-16 11:12:28,029 - INFO - Scoring Task 00576224 pair #1\n",
      "2024-10-16 11:12:28,029 - INFO - Scoring Task 00576224 pair #1\n",
      "2024-10-16 11:12:28,035 - INFO - Final score: 0.0 of 1 (0.0%)\n",
      "2024-10-16 11:12:28,035 - INFO - Final score: 0.0 of 1 (0.0%)\n",
      "2024-10-16 11:12:28,035 - INFO - Final score: 0.0 of 1 (0.0%)\n",
      "2024-10-16 11:12:28,035 - INFO - Final score: 0.0 of 1 (0.0%)\n",
      "2024-10-16 11:12:28,035 - INFO - Final score: 0.0 of 1 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Running the Model\n",
    "# ==========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(task_set='evaluation', NUM_TASKS=1, submission_file_name='submission.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
